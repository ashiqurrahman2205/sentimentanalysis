{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6330af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid', context='talk', palette='Dark2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1023b68",
   "metadata": {},
   "source": [
    "Here we are using a Reddit API wrapper, called `praw`, to loop through the /r/politics subreddit headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91a8bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f0ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='6uiEVMTD9IexgyD_Fd8fvw',\n",
    "                     client_secret='_Kpy5-BXm1CV5sQUbLJ1QLBjiGL8BQ',\n",
    "                     user_agent='ashiqurrahman2205')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba1e87bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = set()\n",
    "#a set for our headlines so we don't get duplicates when running multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09b6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we iterating through the /r/politics subreddit using the API client\n",
    "for submission in reddit.subreddit('politics').new(limit=None):\n",
    "    headlines.add(submission.title)\n",
    "    display.clear_output()\n",
    "    print(len(headlines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d697b6c2",
   "metadata": {},
   "source": [
    "We're iterating over the new posts in /r/politics, and by adding the limit to None we can get up to 1000 headlines. \n",
    "\n",
    "This time we only received 961 headlines.\n",
    "\n",
    "Without some more advanced tricks we can't go past 1000 results since Reddit cuts off at that point. We can run this loop multiple times and keep adding new headlines to our set, or we can implement a streaming version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5479e90f",
   "metadata": {},
   "source": [
    "## Labeling our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2e4c4c",
   "metadata": {},
   "source": [
    "NLTKâ€™s built-in Vader Sentiment Analyzer will give rank a piece of text as positive, negative or neutral using a lexicon of positive and negative words.\n",
    "\n",
    "We can utilize this tool by first creating a Sentiment Intensity Analyzer- SIA to categorize our headlines, then we'll use the polarity_scores method to get the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8793302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "sia = SIA()\n",
    "results = []\n",
    "\n",
    "for line in headlines:\n",
    "    pol_score = sia.polarity_scores(line)\n",
    "    pol_score['headline'] = line\n",
    "    results.append(pol_score)\n",
    "\n",
    "pprint(results[:3], width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139cadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb05fe",
   "metadata": {},
   "source": [
    "The above table consists of four columns from the sentiment scoring: Neural, Negative, Positive and compound. \n",
    "The first three represent the sentiment score percentage of each category in our headline, and the compound single number that scores the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ac0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = 0\n",
    "df.loc[df['compound'] > 0.2, 'label'] = 1\n",
    "df.loc[df['compound'] < -0.2, 'label'] = -1\n",
    "df.head()\n",
    "\n",
    "# We are creating a positive label of 1 if the compound is greater than 0.2, and a label of -1 if compound is less than -0.2. Everything else will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a00526",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['headline', 'label']]\n",
    "df2.to_csv('reddit_headlines_labels.csv', mode='a', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da36313",
   "metadata": {},
   "source": [
    "# Dataset Info and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d9338",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive headlines:\\n\")\n",
    "pprint(list(df[df['label'] == 1].headline)[:5], width=200)\n",
    "\n",
    "print(\"\\nNegative headlines:\\n\")\n",
    "pprint(list(df[df['label'] == -1].headline)[:5], width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b60b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.label.value_counts())\n",
    "\n",
    "print(df.label.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00881313",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "counts = df.label.value_counts(normalize=True) * 100\n",
    "\n",
    "sns.barplot(x=counts.index, y=counts, ax=ax)\n",
    "\n",
    "ax.set_xticklabels(['Negative', 'Neutral', 'Positive'])\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23801e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "\n",
    "example = \"This is an example sentence! However, it isn't a very informative one\"\n",
    "\n",
    "print(word_tokenize(example, language='english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426b3cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73136b46",
   "metadata": {},
   "source": [
    "# Now, we will send the CSV file of our scraped data to Azure blob storage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
